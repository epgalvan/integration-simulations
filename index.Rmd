# Integration Simulations

The present document shows that a per-condition model will often explain data generated from an integrated model better than the integrated model itself.

## Simulating Data

The base simulation will contain no error, and will be based on a modulated effect on inequality-aversion following the instructions outlined in Galvan and Sanfey's paper in *Social Cognition* (2025). In this example, consider a Dictator Game with multiple conditions reflecting one's game partner: a spouse, a generic other, and a rival. The model will state that liking (spouse = 1), (generic other= 0.5), (rival = 0) can modulate these preferences.

### Base Functions

```{r}
payout_maximization = function(endowment, transferred){
  kept = endowment - transferred
  if(kept > 0) {value = log(kept)} else {value = 0}
  maxValue = log(endowment)
  return(value/maxValue)
}
inequality = function(endowment, transferred){
  Equality = (endowment) * 0.5
  maxInequality = Equality - 0
  
  choices = seq(0, endowment)
  Inequality = abs(Equality - choices)
  minInequality = choices[which(Inequality == min(Inequality))][1]
  
  violation = (minInequality - transferred)/maxInequality
  return(1 - (violation)**2)
}
modulator = function(liking){
  return((1 - liking**2))
}
```

### Utility Equation

```{r}
utility = function(theta, phi, modulator, inequality, payout){
  thetaMod = theta * (1-phi) + modulator * phi
  return(thetaMod*payout + (1-thetaMod)*inequality)
}
```

### Generate Predictions

```{r}
generatePredictions = function(params, df){
  prediction = vector('numeric', nrow(df))
  for (k in 1:nrow(df)){
    E = df$Endowment[k]
    L = df$Liking[k]
    Choices = seq(0, E)
    Utility = vector('numeric', length(Choices))
    for (n in 1:length(Choices)){
      Utility[n] = utility(theta = params[1], 
                           phi = params[2], 
                           modulator = modulator(L), 
                           inequality = inequality(E, Choices[n]), 
                           payout = payout_maximization(E, Choices[n]))
    }
    correct_choice = which(Utility == max(Utility))
    if (length(correct_choice) > 1){
      correct_choice = correct_choice[sample(1:length(correct_choice), 1)]
    }
    prediction[k] = Choices[correct_choice]
  }
  return(prediction)
}
```

### Set-Up

Let's get the parameters we want to simulate data for

```{r}
freeParameters = data.frame(theta = rep(seq(0, 1, 0.01), each = 101),
                            phi = rep(seq(0, 1, 0.01), times = 101))

head(freeParameters)
```

And the trialset we want to use

```{r}
trialSet = data.frame(Endowment = rep(seq(10, 100, 10), times = 3),
                      Liking = rep(c(0, 0.5, 1), each = 10))
trialSet
```

### Output the Predictions

```{r}
predictions = data.frame()
for (i in 1:nrow(freeParameters)){
  pars = as.numeric(freeParameters[i, ])
  predictions[i, 1:nrow(trialSet)] = generatePredictions(pars, trialSet)
}
head(predictions)
```

### Objective Function

First let's set up the objective function

```{r}
obj_function = function(params, df, method = "OLS") {
  predicted_utility = vector('numeric', length(df[,1]))
  observed_utility = vector('numeric', length(df[,1]))
  
  Predictions = generatePredictions(params, df)
  
  for (k in 1:nrow(df)){
    E = df$Endowment[k]
    L = df$Liking[k]
    R = df$Transferred[k]
    P = Predictions[k]
    
    predicted_utility[k] = utility(theta = params[1], phi = params[2],
                                   modulator = modulator(L), 
                                   inequality = inequality(E, R), 
                                   payout = payout_maximization(E, R))
    observed_utility[k] =  utility(theta = params[1], phi = params[2],
                                   modulator = modulator(L), 
                                   inequality = inequality(E, R), 
                                   payout = payout_maximization(E, R))
  }
  
  return(sum((predicted_utility - observed_utility)**2))
}
```

### Per Condition Model

We'll call the per condition model the alternative model

```{r}
utilityAlt = function(theta, inequality, payout){
  return(theta*payout + (1-theta)*inequality)
}
generatePredictionsAlt = function(params, df){
  prediction = vector('numeric', nrow(df))
  conditions = levels(factor(df$Liking))
  for (k in 1:nrow(df)){
    E = df$Endowment[k]
    L = df$Liking[k]
    par = params[which(conditions == L)]
    
    Choices = seq(0, E)
    Utility = vector('numeric', length(Choices))
    for (n in 1:length(Choices)){
      Utility[n] = utilityAlt(theta = par,   
                              inequality = inequality(E, Choices[n]), 
                              payout = payout_maximization(E, Choices[n]))
    }
    correct_choice = which(Utility == max(Utility))
    if (length(correct_choice) > 1){
      correct_choice = correct_choice[sample(1:length(correct_choice), 1)]
    }
    prediction[k] = Choices[correct_choice]
  }
  return(prediction)
}
obj_functionAlt = function(params, df, method = "OLS") {
  predicted_utility = vector('numeric', length(df[,1]))
  observed_utility = vector('numeric', length(df[,1]))
  Predictions = generatePredictions(params, df)
  conditions = levels(factor(df$Liking))
  for (k in 1:nrow(df)){
    E = df$Endowment[k]
    L = df$Liking[k]
    R = df$Transferred[k]
    P = Predictions[k]
    par = params[which(conditions == L)]
    
    predicted_utility[k] = utilityAlt(theta = par, 
                                      inequality = inequality(E, P), 
                                      payout = payout_maximization(E, P))
    observed_utility[k] =  utilityAlt(theta = par, 
                                      inequality = inequality(E, R), 
                                      payout = payout_maximization(E, R))
  }
  return(sum((predicted_utility - observed_utility)**2))
}
```

### Initialize for Parameter Recovery

```{r}
library(pracma)

initial_params = c(0.5, 0.5)
lower_bounds = c(0, 0)
upper_bounds = c(1,1)

initial_paramsAlt = c(0.5, 0.5, 0.5)
lower_boundsAlt = c(0, 0, 0)
upper_boundsAlt = c(1, 1, 1)
```

## Proof Goals

At this point we have deduced the choices that the model predicts per each coordinate in parameter space. The goal in what follows is to demonstrate that a less-parsimonious, incorrect model which is fit to conditions will outperform the true data generation process.

Thus, we will compare the 2 parameter model which integrates across conditions to the 3 parameter model which segregates conditions in terms of their ability to explain the data generated by the 2 parameter model (predictions). However, since the predictions do not contain noise, we will noise up the data. The default standard deviation will be 1.

```{r}
noise = function(predictions, sd = 1){
  predictions = predictions + rnorm(predictions, mean = 0, sd = sd)
  predictions = round(predictions)
  predictions[which(predictions < 0)] = 0
  return(predictions)
}
```

We will also provide some intercondition noise, as participants tend to be slightly more consistent within conditions than across

```{r}
intercondition = function(conditions, predictions, sd = 0.5){
  for (condition in levels(factor(conditions))){
    predictions[which(conditions == condition)] = 
      predictions[which(conditions == condition)] + rnorm(1, mean = 0, sd = sd)
  }
  predictions = round(predictions)
  predictions[which(predictions < 0)] = 0
  return(predictions)
}
```

Also, a robust amount of research on these kinds of task indicates that preferences are not evenly distributed across the parameter space.

```{r}
distribution_theta = rnorm(n = 1000000, mean = 0.3, sd = 0.3)
distribution_theta = distribution_theta[-which(distribution_theta < 0 | distribution_theta > 1)]
distribution_theta = round(distribution_theta, 2)
library(ggplot2)
ggplot() + geom_histogram(aes(x = distribution_theta), bins = 15)
```

Around 10 percent of people are selfish, corresponding to a theta \> 0.7 in this equation

```{r}
sum(distribution_theta > 0.7)/length(distribution_theta)
```

Sample the parameter space for the maximum number of observations: 1000

```{r}
n1000 = data.frame(theta = sample(distribution_theta, 1000),
                   phi = sample(seq(0, 1, 0.01), 1000, replace = T))
ggplot(data = n1000) + geom_point(aes(x = theta, y = phi))
```

And save this information so that it can be used in the lower child scripts

```{r}
save.image(file = "C:/Users/DELL/Documents/integration-simulations/index.RData")
```
